{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1 ‚Äî Imports & settings"
      ],
      "metadata": {
        "id": "0tatpCCUcXhH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvlMN0CXcRx0"
      },
      "outputs": [],
      "source": [
        "# 1_imports_and_settings.py\n",
        "# Purpose: import libs, set plotting style and random seed for reproducibility.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix,\\\n",
        " roc_auc_score, roc_curve, accuracy_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Global plotting settings\n",
        "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### centralizing imports and settings makes the notebook easier to read and ensures reproducible results."
      ],
      "metadata": {
        "id": "DyIOfoCWc6is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2 ‚Äî Load data & initial overview\n",
        "1. load_data_and_overview.py\n",
        "2. Load dataset and perform quick checks (head, info,descriptive stats).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "okO1q5GSdKsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/healthcare-dataset-stroke-data.csv')  # update path if necessary"
      ],
      "metadata": {
        "id": "o7hbrqzddTbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick peek\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "oHGdFgL8eLpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Structure and types\n",
        "print(\"===== INFO =====\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "UYMgEaepeQWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical summary\n",
        "print(\"\\n===== DESCRIBE (numeric) =====\")\n",
        "display(df.describe(include=[np.number]).T)"
      ],
      "metadata": {
        "id": "2Dw3jQhPeZKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class balance\n",
        "print(\"\\nStroke value counts:\")\n",
        "display(df['stroke'].value_counts(normalize=False))\n",
        "display(df['stroke'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "SB-MhYmkei1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. df.info() tells you types and non-null counts ‚Äî useful to detect missing values.\n",
        "\n",
        "2. describe() provides mean, std, quartiles for numeric columns (age, bmi, avg_glucose_level).\n",
        "\n",
        "3. Check stroke value counts to see class imbalance (likely very imbalanced ‚Äî few positives)."
      ],
      "metadata": {
        "id": "xAXh2nVee6r0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3 ‚Äî Helper: add_percent for countplots\n"
      ],
      "metadata": {
        "id": "ENsl7rPUfFO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3_plot_helpers.py\n",
        "# A reusable function to show percentage labels above bars.\n",
        "def add_percent(ax, df, x, hue=None):\n",
        "    \"\"\"\n",
        "    Annotate bars with percentage of total rows (not percentage\n",
        "    inside each group).\n",
        "    ax : matplotlib axis containing bars from seaborn.countplot\n",
        "    df : original dataframe\n",
        "    x : x column name (string)\n",
        "    hue: optional hue column name (string) - not used for grouping\n",
        "     calculation here\n",
        "    \"\"\"\n",
        "    total = len(df)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        if height > 0:\n",
        "            ax.text(\n",
        "                p.get_x() + p.get_width() / 2,\n",
        "                height + total * 0.005,\n",
        "                f'{height/total*100:.1f}%',\n",
        "                ha='center',\n",
        "                va='bottom',\n",
        "                fontsize=9\n",
        "            )\n"
      ],
      "metadata": {
        "id": "EGt8VLGCfC4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This function annotates bars with percent of whole dataset. If you want percent within each category (e.g., percent of males that had stroke), we can modify it ‚Äî tell me and I'll provide that variant."
      ],
      "metadata": {
        "id": "0ItzzVCefokD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4 ‚Äî Exploratory plots & interpretation\n",
        "\n",
        "* .  Visual EDA: categorical counts and numeric boxplots vs\n",
        "stroke."
      ],
      "metadata": {
        "id": "2oF0DjANfxBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gender vs stroke\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sns.countplot(x=\"gender\", hue=\"stroke\", data=df)\n",
        "add_percent(ax, df, \"gender\", \"stroke\")\n",
        "plt.title(\"Gender vs Stroke (%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jzd9YaZRfsz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gender vs Stroke: If stroke counts are much higher for one gender proportionally, that suggests a gender association. But always compare proportions (e.g., stroke rate among males vs females) ‚Äî absolute bar heights can be misleading if group sizes differ."
      ],
      "metadata": {
        "id": "g50JtOPVgXJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Age vs stroke (boxplot)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(x=\"stroke\", y=\"age\", data=df)\n",
        "plt.title(\"Age distribution by Stroke (0 = no, 1 = yes)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ftc4XqCXgfby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Age boxplot: Likely shows that median age is higher for stroke patients. Look at median, IQR, and outliers. If older ages cluster in stroke=1, age is an important predictor."
      ],
      "metadata": {
        "id": "wJKOhxcdgpoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypertension vs stroke\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sns.countplot(x=\"hypertension\", hue=\"stroke\", data=df)\n",
        "add_percent(ax, df, \"hypertension\", \"stroke\")\n",
        "plt.title(\"Hypertension vs Stroke (%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S3SxKFYHgkwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heart disease vs stroke\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sns.countplot(x=\"heart_disease\", hue=\"stroke\", data=df)\n",
        "add_percent(ax, df, \"heart_disease\", \"stroke\")\n",
        "plt.title(\"Heart Disease vs Stroke (%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5fRsVbS_gz_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hypertension & Heart disease: Expect higher proportion of stroke among people with hypertension or heart disease. These are known risk factors."
      ],
      "metadata": {
        "id": "aScgFRLJhKu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ever married vs stroke\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sns.countplot(x=\"ever_married\", hue=\"stroke\", data=df)\n",
        "add_percent(ax, df, \"ever_married\", \"stroke\")\n",
        "plt.title(\"Ever Married vs Stroke (%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5CUwGs1-hNm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Ever married: Marriage may correlate with age and socioeconomic factors ‚Äî interpret cautiously."
      ],
      "metadata": {
        "id": "7oXYOKBjhSZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Work type vs stroke\n",
        "plt.figure(figsize=(10,4))\n",
        "ax = sns.countplot(x=\"work_type\", hue=\"stroke\", data=df)\n",
        "add_percent(ax, df, \"work_type\", \"stroke\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.title(\"Work Type vs Stroke (%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Je6w5_PNhY5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Work type: Inspect which job categories have relatively higher stroke proportions. Small categories with high percentages might be noisy (small sample size)."
      ],
      "metadata": {
        "id": "4nP7bIe_hdqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Residence type vs stroke\n",
        "plt.figure(figsize=(6,4))\n",
        "ax = sns.countplot(x=\"Residence_type\", hue=\"stroke\", data=df)\n",
        "add_percent(ax, df, \"Residence_type\", \"stroke\")\n",
        "plt.title(\"Residence Type vs Stroke (%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5X5HknqhhiyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Residence type: Urban vs rural differences ‚Äî consider access to healthcare confounding"
      ],
      "metadata": {
        "id": "UXLzCuYehmTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# avg_glucose_level vs stroke (boxplot)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(x=\"stroke\", y=\"avg_glucose_level\", data=df)\n",
        "plt.title(\"Average Glucose Level by Stroke\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v_9xQ-8Lhq8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bmi vs stroke (boxplot)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(x=\"stroke\", y=\"bmi\", data=df)\n",
        "plt.title(\"BMI by Stroke\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ucsuqy6qhyi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Avg glucose & BMI boxplots: Higher median glucose in stroke patients suggests metabolic link. For BMI, check whether stroke patients have higher/lower medians ‚Äî sometimes underweight or very high BMI show different risks."
      ],
      "metadata": {
        "id": "JKoqCdWLfjsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# smoking status vs stroke\n",
        "plt.figure(figsize=(8,4))\n",
        "ax = sns.countplot(x=\"smoking_status\", hue=\"stroke\", data=df)\n",
        "add_percent(ax, df, \"smoking_status\", \"stroke\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.title(\"Smoking Status vs Stroke (%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CHZs26tLh49y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Smoking status: Look for elevated stroke proportions among current/former smokers. Again watch for small sample sizes in categories like \"Unknown\" or \"never smoked\"."
      ],
      "metadata": {
        "id": "PD6_ctD8h9Uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5 ‚Äî Missing values & imputation\n",
        "1. missing_values_and_imputation.py\n",
        "2.  Check missing values and impute BMI with mean (as you did)"
      ],
      "metadata": {
        "id": "vERM_h7gilPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute BMI with mean (documented)\n",
        "df['bmi'] = df['bmi'].fillna(df['bmi'].mean())\n",
        "print(\"\\nAfter imputing BMI, missing values:\")\n",
        "display(df.isnull().sum())"
      ],
      "metadata": {
        "id": "Ff7N3JWqi9N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Interpretation:\n",
        "\n",
        "* If BMI had missing values only, mean imputation is simple but can underestimate variance. Alternatives: median, KNN imputation, or model-based imputation. Use mean if distribution is symmetric; if skewed prefer median."
      ],
      "metadata": {
        "id": "Kdtqu_n-jTaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6 ‚Äî Encoding categorical variables\n",
        "* Map and one-hot encode categorical variables. Keep dtype=int for clarity."
      ],
      "metadata": {
        "id": "PBr1tOJ4jY-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map small categories to numeric\n",
        "df['gender'] = df['gender'].map({'Male':1, 'Female':0, 'Other':2})\n",
        "df['ever_married'] = df['ever_married'].map({'Yes':1, 'No':0})"
      ],
      "metadata": {
        "id": "2RgmrrfTjjeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode multi-category strings\n",
        "df = pd.get_dummies(df, columns=['work_type','Residence_type',\\\n",
        " 'smoking_status'], drop_first=False, dtype=int)"
      ],
      "metadata": {
        "id": "Pels12OFjtxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop id column\n",
        "if 'id' in df.columns:\n",
        "  df.drop('id', axis=1, inplace=True)\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "P9cvVP1ckFKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Notes:\n",
        "\n",
        "* You used drop_first=False ‚Äî that's fine. If you prefer fewer columns and don't mind collinearity for tree models, keep them as is. For linear models, one-hot with drop_first=True reduces linear dependence; but since you will scale and perhaps regularize, either is acceptable"
      ],
      "metadata": {
        "id": "DkTonCrvkU4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7 ‚Äî Outlier detection"
      ],
      "metadata": {
        "id": "vPYmGsgjkZGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers_iqr(df, column):\n",
        "    \"\"\"\n",
        "    Print count of IQR outliers and the acceptable range.\n",
        "    Returns DataFrame of outlier rows.\n",
        "    \"\"\"\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[column] < lower) | (df[column] > upper)]\n",
        "    print(f\"{column}: outliers = {len(outliers)}\")\n",
        "    print(f\"Acceptable range: {lower:.2f} to {upper:.2f}\")\n",
        "    return outliers\n",
        "\n",
        "out_glucose = detect_outliers_iqr(df, 'avg_glucose_level')\n",
        "out_bmi = detect_outliers_iqr(df, 'bmi')"
      ],
      "metadata": {
        "id": "MdwBbpXYkexm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a few outlier rows for inspection\n",
        "display(out_glucose.head())\n",
        "display(out_bmi.head())"
      ],
      "metadata": {
        "id": "P35-Zuf-kkRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Interpretation & action suggestions:\n",
        "\n",
        "1. Outliers may be true extreme physiological measurements or data errors. Investigate a few rows (age, stroke label) before dropping.\n",
        "\n",
        "2. For modeling, robust scaling or winsorization are alternatives to dropping outliers."
      ],
      "metadata": {
        "id": "2q3WSCshkn4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8 ‚Äî Split, scale, and resample (SMOTE)\n"
      ],
      "metadata": {
        "id": "ZJxFqedWkveb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features and target, split, then scale using train-fit only (avoid data leakage).\n",
        "features = df.drop('stroke', axis=1)\n",
        "target = df['stroke']"
      ],
      "metadata": {
        "id": "0IGXy4VBk9Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split BEFORE scaling to avoid leakage\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2\\\n",
        ", random_state=RANDOM_STATE, stratify=target)"
      ],
      "metadata": {
        "id": "VzVDmUUclB6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize numeric features (fit on train, transform train and test)\n",
        "numeric_cols = ['age', 'avg_glucose_level', 'bmi']\n",
        "scaler = StandardScaler()\n",
        "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
        "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
      ],
      "metadata": {
        "id": "m-Ym7GQGlKLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle class imbalance with SMOTE on the training set ONLY\n",
        "sm = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "YJxhO0erlaxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before SMOTE:\", y_train.value_counts())\n",
        "print(\"After SMOTE:\", y_train_res.value_counts())"
      ],
      "metadata": {
        "id": "UqrjaHIjld-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Why this order?\n",
        "* You must fit the scaler on train set only to avoid leaking distributional information from test set into training (data leakage). Applying SMOTE and scaling should be done after splitting, and SMOTE only on the training set."
      ],
      "metadata": {
        "id": "8d1NnRQyliSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9 ‚Äî Evaluation functio9 ‚Äî Evaluation function"
      ],
      "metadata": {
        "id": "iDu9lq3BlwKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9_evaluation_function.py\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Fit the model on (X_train, y_train), then evaluate on (X_test, y_test).\n",
        "    Print classification report, accuracy, AUC. Plot confusion matrix\n",
        "     and ROC curve.\n",
        "    \"\"\"\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    # Some classifiers may not have predict_proba (e.g., certain SVMs)\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        # fallback to decision_function scaled to [0,1] if available\n",
        "        if hasattr(model, \"decision_function\"):\n",
        "            scores = model.decision_function(X_test)\n",
        "            # scale to 0..1\n",
        "            y_prob = (scores - scores.min()) / (scores.max() -\\\n",
        "            scores.min() + 1e-8)\n",
        "        else:\n",
        "            y_prob = np.zeros(len(y_test))  # not ideal, but prevents crashes\n",
        "\n",
        "    # Reports\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"Result: {model_name}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, digits=3))\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "    # AUC may fail if y_prob constant; guard it\n",
        "    try:\n",
        "        auc = roc_auc_score(y_test, y_prob)\n",
        "        print(f\"AUC: {auc*100:.2f}%\")\n",
        "    except Exception as e:\n",
        "        print(\"AUC: Could not compute (possibly constant predictions).\", e)\n",
        "    print('\\n')\n",
        "    # Confusion matrix plot\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "    # ROC curve plot (if probabilities available)\n",
        "    if np.unique(y_prob).size > 1:\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y_test, y_prob):.3f}\")\n",
        "        plt.plot([0,1], [0,1], '--', color='gray')\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(f\"ROC Curve - {model_name}\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"ROC curve: skipped (no probability variation).\")\n"
      ],
      "metadata": {
        "id": "mToggDuvlr-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Note:\n",
        "*  This function trains the model internally. Call it with the resampled training data and the untouched test set."
      ],
      "metadata": {
        "id": "7H62eNPRmI7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10 ‚Äî Train & evaluate models"
      ],
      "metadata": {
        "id": "wS3HLZycmUWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "log_model = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
        "evaluate_model(log_model, X_train_res, y_train_res, X_test, y_test, \"Logistic Regression\")"
      ],
      "metadata": {
        "id": "dtfgIg7EmjHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Accuracy (81.9%) indicates that the model correctly classified about 82% of all samples.\n",
        "\n",
        "However, Recall for class 1 (stroke = 0.700) shows that it detected 70% of stroke cases ‚Äî quite high for an imbalanced dataset.\n",
        "\n",
        "The Precision (0.171) for stroke is low, meaning it produces many false positives.\n",
        "\n",
        "AUC = 82.03% reflects good overall discrimination ability between stroke and non-stroke cases.\n",
        "\n",
        "Interpretation:\n",
        "The Logistic Regression model is relatively balanced ‚Äî it sacrifices some precision to detect more stroke cases, which is acceptable in a healthcare context where missing a stroke case is more dangerous than a false"
      ],
      "metadata": {
        "id": "Juf-05W0slfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE)\n",
        "evaluate_model(rf_model, X_train_res, y_train_res, X_test, y_test, \"Random Forest\")"
      ],
      "metadata": {
        "id": "zyX-eXmTmns7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Accuracy (90.6%) looks high, but it‚Äôs misleading because the dataset is imbalanced.\n",
        "\n",
        "Recall for class 1 (0.160) means the model only detects 16% of actual stroke cases ‚Äî very poor sensitivity.\n",
        "\n",
        "Precision (0.129) is also low, indicating poor detection of true stroke patients.\n",
        "\n",
        "AUC = 78.07% confirms moderate discrimination power.\n",
        "\n",
        "Interpretation:\n",
        "The Random Forest performs well for the majority class (non-stroke) but fails to detect stroke cases effectively ‚Äî not suitable for medical diagnosis as is."
      ],
      "metadata": {
        "id": "-qcmJU8osuCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE)\n",
        "evaluate_model(xgb_model, X_train_res, y_train_res, X_test, y_test, \"XGBoost\")"
      ],
      "metadata": {
        "id": "EHT-t8A1mrfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Similar to Random Forest with Accuracy ‚âà 90.7%,\n",
        "but Recall (0.180) and Precision (0.143) show slightly better performance on stroke detection.\n",
        "\n",
        "AUC = 78.34% ‚Äî slightly higher than Random Forest, meaning marginally better distinction between classes.\n",
        "\n",
        "Interpretation:\n",
        "XGBoost improves slightly over Random Forest, but still struggles with the minority (stroke) class. Further tuning or resampling (e.g., SMOTE during training) is needed."
      ],
      "metadata": {
        "id": "AkCsxRWZs0kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation Summary\n",
        "Among the three models tested (Logistic Regression, Random Forest, XGBoost),  \n",
        "**Logistic Regression** achieved the best recall (0.70) and strong AUC (82%),  \n",
        "making it the most suitable model for detecting stroke cases.  \n",
        "While Random Forest and XGBoost had higher accuracy, their recall for stroke cases was poor,  \n",
        "which is critical in medical predictions.\n"
      ],
      "metadata": {
        "id": "YLIFHbivu_Xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11 ‚ÄîSave the trained model and preprocessing objects\n"
      ],
      "metadata": {
        "id": "hYxhT4kzoo7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib # import librarary to use it save the project"
      ],
      "metadata": {
        "id": "4apd70bFpQjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the trained Logistic Regression model as a binary file (.pkl)\n",
        "# This allows us to reload and use the model later without retraining.\n",
        "joblib.dump(log_model, 'best_model_logistic_regression.pkl')"
      ],
      "metadata": {
        "id": "5NsBCEiQoqzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the feature names used during model training\n",
        "# Ensures that new data passed to the model will have the same structure.\n",
        "joblib.dump(list(features.columns), 'model_features.pkl')"
      ],
      "metadata": {
        "id": "A1HHKjYwplnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fitted scaler to apply the exact same scaling on future data\n",
        "# This is critical to maintain consistency between training and inference.\n",
        "joblib.dump(scaler, 'scaler.pkl')"
      ],
      "metadata": {
        "id": "EkN1yxFYqslv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"‚úÖ Model, features, and scaler saved successfully!\")"
      ],
      "metadata": {
        "id": "KMv7Ltq1qxTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Colab file utility\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "k6G2Th-Yse2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download each saved file to your laptop\n",
        "# These files can later be loaded in VS Code or any Python environment.\n",
        "files.download('best_model_logistic_regression.pkl')\n",
        "files.download('model_features.pkl')\n",
        "files.download('scaler.pkl')\n",
        "\n",
        "print(\"üìÇ All files downloaded successfully to your computer!\")"
      ],
      "metadata": {
        "id": "PFt-rFKesi_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stroke Prediction Web App using Gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "b5QVqOnkwMuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Load Trained Model, Scaler, and Feature Names\n",
        "# =====================================================\n",
        "model = joblib.load(\"best_model_logistic_regression.pkl\")   # Trained ML model\n",
        "features = joblib.load(\"model_features.pkl\")                # List of features used during training\n",
        "scaler = joblib.load(\"scaler.pkl\")                          # Scaler used for normalization)"
      ],
      "metadata": {
        "id": "_yhrjV9YwPsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# =========================\n",
        "# Load model, scaler, and features\n",
        "# =========================\n",
        "model = joblib.load(\"best_model_logistic_regression.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "features = joblib.load(\"model_features.pkl\")\n",
        "\n",
        "# =========================\n",
        "# Prediction Function\n",
        "# =========================\n",
        "def predict_stroke(age, hypertension, heart_disease, avg_glucose_level, bmi,\n",
        "                   gender, ever_married, work_type, Residence_type, smoking_status):\n",
        "\n",
        "    # Build input DataFrame\n",
        "    data = pd.DataFrame([{\n",
        "        \"age\": age,\n",
        "        \"hypertension\": hypertension,\n",
        "        \"heart_disease\": heart_disease,\n",
        "        \"avg_glucose_level\": avg_glucose_level,\n",
        "        \"bmi\": bmi,\n",
        "        \"gender\": gender,\n",
        "        \"ever_married\": ever_married,\n",
        "        \"work_type\": work_type,\n",
        "        \"Residence_type\": Residence_type,\n",
        "        \"smoking_status\": smoking_status\n",
        "    }])\n",
        "\n",
        "    # One-hot encode\n",
        "    data_ready = pd.get_dummies(data)\n",
        "\n",
        "    # Reindex to match training features\n",
        "    data_ready = data_ready.reindex(columns=features, fill_value=0)\n",
        "\n",
        "    # Scale numeric features\n",
        "    numeric_cols = [\"age\", \"avg_glucose_level\", \"bmi\"]\n",
        "    data_ready[numeric_cols] = scaler.transform(data_ready[numeric_cols])\n",
        "\n",
        "    # Make prediction\n",
        "    pred_prob = model.predict_proba(data_ready)[0][1]\n",
        "    prediction = \"Stroke Risk Detected ‚ö†Ô∏è\" if pred_prob > 0.5 else \"No Stroke Risk ‚úÖ\"\n",
        "\n",
        "    return f\"**{prediction}**\\n\\nProbability: {pred_prob:.2%}\"\n",
        "\n",
        "# =========================\n",
        "# Gradio UI\n",
        "# =========================\n",
        "with gr.Blocks(title=\"Stroke Risk Predictor - Designed by Islam Abdul Rahim Mohamed\") as demo:\n",
        "\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    <h2 style=\"text-align:center;\">üß† Stroke Prediction Web App</h2>\n",
        "    <p style=\"text-align:center;\">Provide patient information below to estimate stroke risk.</p>\n",
        "    \"\"\", elem_id=\"header\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            age = gr.Number(label=\"Age\", value=45)\n",
        "            hypertension = gr.Dropdown([\"0\", \"1\"], label=\"Hypertension (0 = No, 1 = Yes)\", value=\"0\")\n",
        "            heart_disease = gr.Dropdown([\"0\", \"1\"], label=\"Heart Disease (0 = No, 1 = Yes)\", value=\"0\")\n",
        "            avg_glucose_level = gr.Number(label=\"Average Glucose Level\", value=90.0)\n",
        "            bmi = gr.Number(label=\"BMI\", value=25.0)\n",
        "\n",
        "        with gr.Column():\n",
        "            gender = gr.Dropdown([\"Male\", \"Female\", \"Other\"], label=\"Gender\", value=\"Male\")\n",
        "            ever_married = gr.Dropdown([\"Yes\", \"No\"], label=\"Ever Married\", value=\"No\")\n",
        "            work_type = gr.Dropdown([\"Private\", \"Self-employed\", \"Govt_job\", \"Children\", \"Never_worked\"], label=\"Work Type\", value=\"Private\")\n",
        "            Residence_type = gr.Dropdown([\"Urban\", \"Rural\"], label=\"Residence Type\", value=\"Urban\")\n",
        "            smoking_status = gr.Dropdown([\"formerly smoked\", \"never smoked\", \"smokes\", \"Unknown\"], label=\"Smoking Status\", value=\"never smoked\")\n",
        "\n",
        "    predict_button = gr.Button(\"üîç Predict Stroke Risk\", variant=\"primary\")\n",
        "    output = gr.Markdown(label=\"Result\")\n",
        "\n",
        "    predict_button.click(\n",
        "        predict_stroke,\n",
        "        inputs=[age, hypertension, heart_disease, avg_glucose_level, bmi,\n",
        "                gender, ever_married, work_type, Residence_type, smoking_status],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    # Footer with signature\n",
        "    gr.Markdown(\"\"\"\n",
        "    ---\n",
        "    <p style=\"text-align:center; font-size:14px; color:gray;\">\n",
        "    ü©∫ <b>Model:</b> Logistic Regression &nbsp;&nbsp;|&nbsp;&nbsp; üì¶ <b>Built with:</b> Scikit-learn & Gradio\n",
        "    </p>\n",
        "    <p style=\"text-align:center; font-family:'Brush Script MT', cursive; font-size:20px; color:#1a202c;\">\n",
        "    Designed by <span style=\"color:#2b6cb0;\">Islam Abdul Rahim Mohamed</span>\n",
        "    </p>\n",
        "    \"\"\")\n",
        "\n",
        "# =========================\n",
        "# Launch the app\n",
        "# =========================\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "fHckfnX3AbT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Model Inference (Prediction)\n",
        "The saved model can be reloaded and used to predict stroke risk for new patients.\n"
      ],
      "metadata": {
        "id": "XDj3-zUAvbHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Technical Notes\n",
        "- SMOTE was applied **only on training data** to prevent data leakage.  \n",
        "- Test data was kept untouched for fair evaluation.  \n",
        "- Models were evaluated using Accuracy, Recall, Precision, F1-score, and AUC.\n"
      ],
      "metadata": {
        "id": "0OvujVOdvhu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion & Future Work\n",
        "The Logistic Regression model performed best, but there is room for improvement.  \n",
        "Future work could include:\n",
        "- Hyperparameter tuning using GridSearchCV.  \n",
        "- Feature importance analysis to understand which health factors matter most.  \n",
        "- Trying advanced ensemble methods or neural networks.  \n",
        "- Deploying the model as a web or mobile app for clinical screening.\n"
      ],
      "metadata": {
        "id": "niwvuEQpvjoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project Summary\n",
        "This project explored stroke prediction using multiple machine learning models.  \n",
        "After data cleaning, preprocessing, and resampling with SMOTE,  \n",
        "Logistic Regression achieved the best performance (AUC = 82%, Recall = 0.70).  \n",
        "The model was saved for deployment and future analysis.\n"
      ],
      "metadata": {
        "id": "X0Q1UWfLvqlb"
      }
    }
  ]
}